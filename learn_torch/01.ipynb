{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create known parameters\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# in linear reqgression, weight is 'b' and bias is 'a' from\n",
    "# Y = a + bX\n",
    "\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y = weight * X + bias\n",
    "#unsqueeze adds one more bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting in training(60-80) and test(20-40) set\n",
    "\n",
    "train_split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test,y_test = X[train_split:], y[train_split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train, train_lables=y_train, test_data=X_test, test_labels=y_test, predictions=None):\n",
    "    #plots training daat, test data and compare predictiosn\n",
    "    plt.figure(figsize=(10,7))\n",
    "    #Plot training data in blue\n",
    "    plt.scatter(train_data, train_lables, c=\"b\", s=4, label=\"Training data\")\n",
    "    #Plot test data in green\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "    # plot predictions if exist\n",
    "\n",
    "    if predictions is not None:\n",
    "        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Prediction\")\n",
    "    #legend\n",
    "        \n",
    "    plt.legend(prop={\"size\": 14});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model with param as A and B\n",
    "# Create linear regression model class\n",
    "\n",
    "class LinearRegressionModel(nn.Module): #inherits from nn.module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.bias = nn.Parameter(torch.randn(1, requires_grad=True,dtype=torch.float))\n",
    "\n",
    "\n",
    "    # Forward Method to define computation in Model\n",
    "    # x is input data\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        return self.weights * x + self.bias\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What model does:\n",
    "*  start with random values of weight and bias\n",
    "* Look at training data and adjust the random vaues to get closer to ideal values\n",
    "\n",
    "\n",
    "How it does?\n",
    "1. Gradient Descent\n",
    "2. Back propogation\n",
    "\n",
    "\n",
    "\n",
    "### TOrch model building essentials\n",
    "\n",
    "* torch.nn - contains all building blocks for Neural Network\n",
    "\n",
    "* toech.nn.Parameter - Stores tensors that can be used with nn.Module. If requires_grad=True gradients (used for updating model parameters via gradient descent) are calculated automatically, this is often referred to as \"autograd\".\n",
    "\n",
    "* torch.nn.Module - The base class for all neural network modules, all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass nn.Module. Requires a f`orward()` method be implemented.\n",
    "\n",
    "* torch.optim - Contains various optimization algorithms (these tell the model parameters stored in nn.Parameter how to best change to improve gradient descent and in turn reduce the loss).\n",
    "\n",
    "* `def forward()` All nn.Module subclasses require you to override forward(), this is what happens in forward computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(57)\n",
    "\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "list(model_0.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using torch.inference_mode()\n",
    "# When we pass data to our model, it'll go through the model's forward() method and produce a result using the computation we've defined.\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_0(X_test)\n",
    "\n",
    "y_preds\n",
    "\n",
    "#inference reduces gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(predictions=y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea of training is to move from randomm param to some known params\n",
    "\n",
    "# One way to measure how poor or wrong your model predictions are is to use a loss func\n",
    "\n",
    "# Loss func is also called cost func or criterion\n",
    "\n",
    "# 5:45:41\n",
    "# loss func is measure of how wrong your predictions are compared to OG\n",
    "\n",
    "# optimizer: Takes into account loss of model and adjust the params\n",
    "\n",
    "# For torch, we need a training loop and a testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Loss function (pick fron many)\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "\n",
    "# setup an optimizer (pick fron many)\n",
    "# stochastic gradient descent\n",
    "\n",
    "# LR := learning rate , most imp param\n",
    "# smol Lr, smol change in param\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), \n",
    "                            lr=0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop (and testing) in Torch\n",
    "\n",
    "0. Loop through data\n",
    "1. Forward pass (involve data moving through model's `forward()` func) := called as forward propogation\n",
    "2. Calculate the loss (compare forward pass prediction to ground truth lable)\n",
    "3. OPtimizer zero grad\n",
    "4. Loss backward - move backwards through network to calculate gradients of each param of model wrt loss (**back propogation**)\n",
    "\n",
    "5. Optimizer step := use to adjust model's param to try and improve loss  (**gradient descent**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(57)\n",
    "\n",
    "# epoch is one loop through data, hyperparameter\n",
    "epochs = 250\n",
    "\n",
    "epoch_count = []\n",
    "loss_values = []\n",
    "test_loss_values = []\n",
    "\n",
    "# Training\n",
    "\n",
    "# 0.\n",
    "for epoch in range(epochs):\n",
    "    #train mode sets all param that require gradient to gradient\n",
    "    model_0.train()\n",
    "\n",
    "    # 1.\n",
    "    y_pred = model_0(X_train)\n",
    "\n",
    "    # 2.\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    # 3. \n",
    "    optimizer.zero_grad() # make it zero, because it accumulates garbage value\n",
    "\n",
    "\n",
    "    # 4.\n",
    "    loss.backward()\n",
    "\n",
    "    # 5.\n",
    "\n",
    "    optimizer.step() \n",
    "\n",
    "    # tessting\n",
    "    model_0.eval() # turns off gradient tracking, not needed for testing\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        #1. do forward pass\n",
    "        test_pred = model_0(X_test)\n",
    "\n",
    "        #2. calculate test loss\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "    \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        epoch_count.append(epoch)\n",
    "        loss_values.append(loss)\n",
    "        test_loss_values.append(test_loss)\n",
    "        \n",
    "        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n",
    "        # print state\n",
    "        print(model_0.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.state_dict()\n",
    "\n",
    "np.array(torch.tensor(loss_values).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count, loss_values, test_loss_values\n",
    "\n",
    "plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label=\"Traain Loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"TEst Loss\")\n",
    "plt.title(\"training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    y_preds_new = model_0(X_test)\n",
    "\n",
    "plot_predictions(predictions=y_preds_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "### Save a model\n",
    "\n",
    "1. `torch.save()` := saves in python .pkl format\n",
    "2. `torch.load()` := load a saved torch model\n",
    "3. `torch.nn.Module.load_state_dict()` := loadds model's saved dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# create model path\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# create save path\n",
    "MODEL_NAME = \"01_torch_model.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "\n",
    "# save model\n",
    "\n",
    "print(MODEL_SAVE_PATH)\n",
    "torch.save(obj=model_0.state_dict(), f=MODEL_SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "loaded_model_0 = LinearRegressionModel()\n",
    "print(loaded_model_0.state_dict())\n",
    "\n",
    "loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
    "print(loaded_model_0.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "\n",
    "loaded_model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    loaded_model_preds = loaded_model_0(X_test)\n",
    "\n",
    "loaded_model_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModelV2(nn.Module):\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "\n",
    "        # use this for creating model param\n",
    "        # also called linear transform, fully connected layer, dense layer\n",
    "        self.linear_layer = nn.Linear(in_features=1,\n",
    "                                      out_features=1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor ) -> torch.Tensor:\n",
    "        return self.linear_layer(x)\n",
    "    \n",
    "\n",
    "torch.manual_seed(57)\n",
    "model_1 = LinearRegressionModelV2()\n",
    "model_1, model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "# loss func\n",
    "loss_fn = nn.L1Loss() #same as MAE\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "torch.manual_seed(57)\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_1.train()\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model_1(X_train)\n",
    "\n",
    "\n",
    "    # calculate loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # optimize zero grad, aka remove waste\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Back propogation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # Testing\n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_1(X_test)\n",
    "\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "    \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n",
    "        print(model_1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Turn model into evaluation mode\n",
    "model_1.eval()\n",
    "\n",
    "# Make predictions on the test data\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_1(X_test)\n",
    "y_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(predictions=y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path \n",
    "MODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Save the model state dict \n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Instantiate a fresh instance of LinearRegressionModelV2\n",
    "loaded_model_1 = LinearRegressionModelV2()\n",
    "\n",
    "# Load model state dict \n",
    "loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Evaluate loaded model\n",
    "loaded_model_1.eval()\n",
    "with torch.inference_mode():\n",
    "    loaded_model_1_preds = loaded_model_1(X_test)\n",
    "y_preds == loaded_model_1_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
